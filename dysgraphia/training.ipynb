{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a77807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "dataset_path = \"Handwriting\"  # Replace with your dataset folder path\n",
    "categories = [\"Low Risk for Dysgraphia\", \"High Risk for Dysgraphia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing parameters\n",
    "img_width, img_height = 150, 150  # Resize all images\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment words in an image\n",
    "def segment_words(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply binary thresholding\n",
    "    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    word_segments = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Filter small noise\n",
    "        if w > 20 and h > 20:  # Adjust thresholds as needed\n",
    "            word = image[y:y+h, x:x+w]\n",
    "            word_segments.append(word)\n",
    "    return word_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and label the images\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    label = categories.index(category)  # 0 for \"Low Potential Dysgraphia\", 1 for \"Potential Dysgraphia\"\n",
    "\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        try:\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Segment the words in the image\n",
    "            word_segments = segment_words(img)\n",
    "\n",
    "            for word in word_segments:\n",
    "                # Resize each segmented word to fit model input shape\n",
    "                word_resized = cv2.resize(word, (img_width, img_height))\n",
    "                \n",
    "                # Convert to grayscale and normalize\n",
    "                word_gray = cv2.cvtColor(word_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Automatically invert if text is white on black\n",
    "                mean_intensity = np.mean(word_gray)\n",
    "                if mean_intensity < 127:\n",
    "                    # Likely white text on black background â†’ invert\n",
    "                    word_gray = cv2.bitwise_not(word_gray)\n",
    "\n",
    "                word_normalized = word_gray / 255.0\n",
    "                \n",
    "                # Reshape for CNN (150x150x1)\n",
    "                word_normalized = np.expand_dims(word_normalized, axis=-1)\n",
    "\n",
    "                # Append the data and labels\n",
    "                data.append(word_normalized)\n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
