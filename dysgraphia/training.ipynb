{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a77807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "dataset_path = \"Handwriting\"  # Replace with your dataset folder path\n",
    "categories = [\"Low Risk for Dysgraphia\", \"High Risk for Dysgraphia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing parameters\n",
    "img_width, img_height = 150, 150  # Resize all images\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment words in an image\n",
    "def segment_words(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply binary thresholding\n",
    "    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    word_segments = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Filter small noise\n",
    "        if w > 20 and h > 20:  # Adjust thresholds as needed\n",
    "            word = image[y:y+h, x:x+w]\n",
    "            word_segments.append(word)\n",
    "    return word_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and label the images\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(dataset_path, category)\n",
    "    label = categories.index(category)  # 0 for \"Low Potential Dysgraphia\", 1 for \"Potential Dysgraphia\"\n",
    "\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        try:\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            # Segment the words in the image\n",
    "            word_segments = segment_words(img)\n",
    "\n",
    "            for word in word_segments:\n",
    "                # Resize each segmented word to fit model input shape\n",
    "                word_resized = cv2.resize(word, (img_width, img_height))\n",
    "                \n",
    "                # Convert to grayscale and normalize\n",
    "                word_gray = cv2.cvtColor(word_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Automatically invert if text is white on black\n",
    "                mean_intensity = np.mean(word_gray)\n",
    "                if mean_intensity < 127:\n",
    "                    # Likely white text on black background â†’ invert\n",
    "                    word_gray = cv2.bitwise_not(word_gray)\n",
    "\n",
    "                word_normalized = word_gray / 255.0\n",
    "                \n",
    "                # Reshape for CNN (150x150x1)\n",
    "                word_normalized = np.expand_dims(word_normalized, axis=-1)\n",
    "\n",
    "                # Append the data and labels\n",
    "                data.append(word_normalized)\n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa03486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "data = np.array(data, dtype=\"float32\")  # No need to divide again here, it's already done\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22de13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01839e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 1)),  # Shape: (150, 150, 1)\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c03ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"handwriting_dysgraphia_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83721140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Data distribution visualization\n",
    "category_counts = [len(os.listdir(os.path.join(dataset_path, category))) for category in categories]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=categories, y=category_counts, palette=\"viridis\")\n",
    "plt.title(\"Distribution of Samples in Dataset\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample images\n",
    "def show_random_samples(data, labels, category_names, num_samples=5):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    indices = random.sample(range(len(data)), num_samples)\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(data[idx])\n",
    "        plt.title(category_names[labels[idx]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display random images from each class\n",
    "show_random_samples(data, labels, categories, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation accuracy/loss plots\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
